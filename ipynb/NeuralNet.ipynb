{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from random import random, seed\n",
    "from math import exp, log as ln\n",
    "\n",
    "\n",
    "class InvalidArgumentException(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "class NeuralNet:\n",
    "    \"\"\"\n",
    "    Learning rate determines the magnitude of updates between each epoch\n",
    "    Threshold sets the minimum value a neuron must obtain to activate\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, threshold=0):\n",
    "        self.layers = []\n",
    "        self.threshold = threshold\n",
    "    \n",
    "    \n",
    "    def set_layers(self, layers):\n",
    "        \"\"\"\n",
    "        Create #l layers of #n nodes (#n is defined during Layer initialization).\n",
    "        \"\"\"\n",
    "        if isinstance(layers, list):\n",
    "            for layer in layers:\n",
    "                if not isinstance(layer, Layer):\n",
    "                    raise InvalidArgumentException(\"NeuralNet.set_layers only accepts a list of Layer objects\")\n",
    "                else:\n",
    "                    self.layers.append(layer)\n",
    "        self.__initialize()\n",
    "                    \n",
    "    \n",
    "    def __initialize(self):\n",
    "        \"\"\"\n",
    "        Before training, we need to assign random weights (-1, 1) to every neural pathway\n",
    "        \n",
    "        In a fully connected network, this means creating a weighted path between each neuron in one layer to each\n",
    "        neuron in the next.\n",
    "        \"\"\"\n",
    "        \n",
    "        for l in xrange(0, len(self.layers) - 1):\n",
    "            for n in xrange(0, len(self.layers[l].nodes)):\n",
    "                self.layers[l].nodes[n].set_weights(\n",
    "                    [random() * 2 - 1 for weight in xrange(0, len(self.layers[l + 1].nodes))]\n",
    "                )\n",
    "                \n",
    "                \n",
    "    def __ingest(self, data):\n",
    "        \"\"\"\n",
    "        Loads data into the input layer.  The input layer will always have 1 extra node which serves as a bias\n",
    "        \"\"\"\n",
    "        for d in range(0, len(data)):\n",
    "            self.layers[0].nodes[d].value = data[d]\n",
    "            \n",
    "            \n",
    "    def __forward_propogate(self):\n",
    "        \"\"\"\n",
    "        Activates neurons in each network on forward pass if the sum of weights times values of the previous layer\n",
    "        exceeds threshold\n",
    "        \"\"\"\n",
    "        \n",
    "        for l in xrange(1, len(self.layers)):\n",
    "            if self.layers[l].activation_function is not \"softmax\":\n",
    "                for n in xrange(0, len(self.layers[l].nodes)):\n",
    "                    neuron = 0.0\n",
    "                    for x in self.layers[l - 1].nodes:\n",
    "                        neuron += x.weights[n] * x.value\n",
    "\n",
    "                    self.layers[l].nodes[n].set_value(self.layers[l].activate(neuron - self.threshold))\n",
    "            else:\n",
    "                pass\n",
    "        \n",
    "        \n",
    "    def __back_propogate(self, actual):\n",
    "        \"\"\"\n",
    "        Distributes error to weights on backward pass\n",
    "        \"\"\"\n",
    "        for o in xrange(0, len(self.layers[-1].nodes)):  # Output layer's error is simply y minus y_hat\n",
    "            self.layers[-1].nodes[o].set_error(actual[o] - self.layers[-1].nodes[o].value)\n",
    "        for l in xrange(len(self.layers) - 1, 0, -1):\n",
    "            if self.layers[l].activation_function is not \"softmax\":\n",
    "                for n in self.layers[l - 1].nodes:\n",
    "                    error = 0.0\n",
    "                    for w in xrange(0, len(n.weights)):\n",
    "                        error += n.weights[w] * self.layers[l].nodes[w].error\n",
    "                        n.set_error(self.layers[l].error_function(n.value) * error)\n",
    "                    new_weights = []\n",
    "                    for w in xrange(0, len(n.weights)):\n",
    "                        new_weights.append(n.weights[w] + self.lr * self.layers[l].nodes[w].error * n.value)\n",
    "                    n.set_weights(new_weights)\n",
    "            else:\n",
    "                pass\n",
    "    \n",
    "    \n",
    "    def train(self, data, epochs=100, learning_rate=0.001, verbose=False):\n",
    "        self.lr = learning_rate\n",
    "        for e in xrange(0, epochs):\n",
    "            for d in range(0, len(data)):\n",
    "                self.__ingest(data[d][0])\n",
    "                self.__forward_propogate()\n",
    "                self.__back_propogate(data[d][1])\n",
    "            if verbose:\n",
    "                print(\"Epoch {} out of {}\".format(e, epochs))\n",
    "                \n",
    "                \n",
    "    def predict(self, data):\n",
    "        self.__ingest(data[0])\n",
    "        self.__forward_propogate()\n",
    "        return [node.value for node in self.layers[-1].nodes]\n",
    "    \n",
    "            \n",
    "    \n",
    "class Layer:\n",
    "    def __init__(self, role, nodes, activation_function=None):\n",
    "        self.role = self.__set_role(role)\n",
    "        self.nodes = self.__set_nodes(nodes)\n",
    "        self.activation_function = activation_function # for reference by name\n",
    "        self.activate = self.__set_activation(activation_function)\n",
    "        self.error_function = self.__set_error_function(activation_function)\n",
    "        \n",
    "        \n",
    "    def __set_role(self, role):\n",
    "        \"\"\"\n",
    "        Establishes how a particular layer should behave within the net.  Input and Output layers have special\n",
    "        conditions that must be taken into consideration.\n",
    "        \n",
    "        TODO: Implement convolutional, pooling, and dropout layers\n",
    "        \"\"\"\n",
    "        return {\n",
    "            'input': 'Input',\n",
    "            'hidden': 'Hidden',\n",
    "            'output': 'Output',\n",
    "            #'convolutional': 'Convolutional',\n",
    "            #'pooling': 'Pooling',\n",
    "            #'dropout': 'Dropout'\n",
    "        }[role]    \n",
    "    \n",
    "    \n",
    "    def __set_activation(self, af):\n",
    "        \"\"\"\n",
    "        Sets how information will be passed during forward propogation\n",
    "        TODO:  Implement softmax function for output layer.\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.role == \"Input\":\n",
    "            return None\n",
    "        return {\n",
    "            \"LReLU\": lambda x: x if x > 0 else 0.01 * x,\n",
    "            \"softmax\": lambda x: self.__softmax(x)\n",
    "        }[af]\n",
    "    \n",
    "    \n",
    "    def __set_error_function(self, af):\n",
    "        \"\"\"\n",
    "        Sets how information will be passed during back propogation\n",
    "        \"\"\"\n",
    "        if self.role == \"Input\":\n",
    "            return None\n",
    "        return {\n",
    "            \"LReLU\": lambda x: 1 if x > 0 else 0.01,\n",
    "            \"softmax\": lambda x: self.__cross_entropy(x)\n",
    "        }[af]\n",
    "    \n",
    "    \n",
    "    def __softmax(self):\n",
    "        exp_n = [exp(node.value) for node in self.nodes]\n",
    "        self.softmax = [n / sum(exp_n) for n in exp_n]\n",
    "    \n",
    "    \n",
    "    def __cross_entropy(self, i):\n",
    "        pass\n",
    "    \n",
    "\n",
    "    def __set_nodes(self, nodes):\n",
    "        \"\"\"\n",
    "        Initialize nodes within a layer.  If layer is the input layer, add one extra node whose value will\n",
    "        remain constant as 1.  This bias node prevents anchoring our data to 0 and allows for a better fit. \n",
    "        \"\"\"\n",
    "        \n",
    "        if self.role is 'Input':\n",
    "            return [Node() for node in xrange(0, nodes + 1)]\n",
    "        else:\n",
    "            return [Node() for node in xrange(0, nodes)]\n",
    "    \n",
    "    \n",
    "class Node:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Creates a node with a default value of 1\n",
    "        \"\"\"\n",
    "        self.value = 1\n",
    "        self.error = 1000\n",
    "        \n",
    "        \n",
    "    def set_value(self, value):\n",
    "        self.value = value\n",
    "        \n",
    "        \n",
    "    def set_weights(self, weights):\n",
    "        self.weights = weights\n",
    "        \n",
    "        \n",
    "    def set_error(self, error):\n",
    "        self.error = error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: 0.408163265306\n",
      "\n",
      "After : 0.959183673469\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from numpy import array\n",
    "from random import shuffle\n",
    "\n",
    "data = load_iris()\n",
    "target = data['target'].tolist()\n",
    "\n",
    "actual = []\n",
    "\n",
    "for i in target:\n",
    "    actual.append([1 if i == j else 0 for j in xrange(0,3)])\n",
    "\n",
    "dataset = zip(data['data'].tolist(), actual)\n",
    "shuffle(dataset)\n",
    "train = dataset[:101]\n",
    "test = dataset[101:]\n",
    "\n",
    "nn = NeuralNet()\n",
    "nn.set_layers([\n",
    "        Layer('input', 4),\n",
    "        Layer('hidden', 10, \"LReLU\"),\n",
    "        Layer('output', 3, \"LReLU\")\n",
    "    ])\n",
    "\n",
    "\n",
    "score = 0.0\n",
    "for i in test:\n",
    "    p = nn.predict(i)\n",
    "    if p.index(max(p)) == i[1].index(1):\n",
    "        score += 1\n",
    "        \n",
    "        \n",
    "# Expect a value around 0.333, since there is a\n",
    "# 1 in 3 chance to randomly guess correctly\n",
    "print(\"Before: {}\\n\".format(score / len(test)))\n",
    "\n",
    "nn.train(train, epochs = 1000, learning_rate=0.01)\n",
    "\n",
    "score = 0.0\n",
    "for i in test:\n",
    "    p = nn.predict(i)\n",
    "    if p.index(max(p)) == i[1].index(1):\n",
    "        score += 1\n",
    "        \n",
    "# Expect a value much closer to 1.0, since, if\n",
    "# all went well, the neural net now knows a thing\n",
    "# or two about irises\n",
    "print(\"After : {}\\n\".format(score / len(test)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
